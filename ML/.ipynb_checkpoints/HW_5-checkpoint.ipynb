{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выращиваем решающее дерево своими руками\n",
    "\n",
    "### OzonMasters, \"Машинное обучение 1\"\n",
    "\n",
    "В этом ноутбуке вам предлагается реализовать решающее дерево."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "Для начала импортируем библиотеки, которые нам понадобятся в дальнейшем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решающее дерево\n",
    "\n",
    "Вспомним, что такое решающее дерево.\n",
    "\n",
    "Решающее дерево - алгоритм машинного обучения, задающийся специальным графом-деревом. В данном задании мы будем использовать бинарное дерево. Каждая внутренняя вершина такого дерева соответствует функции предикату $\\mathbb{I}[x_{\\alpha} \\geq \\beta]$. Каждая листовая вершина соответствует некоторому значению ответа, которое будет выдавать алгоритм (вещественное число в случае регрессии, номер класса или вектор вероятностей в случае классификации).\n",
    "\n",
    "На этапе обучения нам необходимо построить само дерево, а также выбрать $\\alpha$ и $\\beta$ для каждой внутренней вершины и метку прогноза для каждой листовой вершины. Задача построения \"наилучшего\" дерева (например того, которое не совершает ошибок и имеет минимальное число вершин) является NP-полной, поэтому при построении деревьев на практике приходиться использовать жадные алгоритмы.\n",
    "\n",
    "На этапе применения все объекты пропускаются через дерево. Изначально, для каждого объекта вычисляется значение функции-предиката корневой вершины. Если оно равно нулю, то алгоритм переходит в левую дочернюю вершину, иначе в правую. Затем вычисляется значение предиката в новой вершине и делается переход или влево, или вправо. Процесс продолжается, пока не будет достигнута листовая вершина. Алгоритм возвращает то значение, которое будет приписано этой вершине.\n",
    "\n",
    "### Выбор $\\alpha$ и $\\beta$\n",
    "\n",
    "На этапе построения дерева мы будем выбирать предикаты для каждой новой вершины, максимизируя функционал качества для разбиения вершины на два поддерева, который можно записать в следующем виде:\n",
    "$$ Q(R, \\alpha, \\beta) = H(R) - \\frac{|R_l|}{|R|} H(R_l) - \\frac{|R_r|}{|R|} H(R_r) $$\n",
    "\n",
    "* $H$ - критерий информативности\n",
    "* $R$ - объекты в текущей вершине\n",
    "* $R_r$ - объекты, попадающие в правое поддерево\n",
    "* $R_l$ - объекты, попадающие в левое поддерево\n",
    "\n",
    "Например, критерий информативности Джини для задачи классификации задаётся следующей оптимизационной задачей:\n",
    "\n",
    "\n",
    "$$H(R) = \\min_{c: \\; \\sum_{k=1}^K c_k = 1} \\frac{1}{|R|} \\sum_{(x_i, y_i) \\in R} \\sum_{k=1}^K (c_k - \\mathbb{I}[y_i=k])^2$$\n",
    "\n",
    "Решение задачи можно найти аналитически:\n",
    "$$H(R) = \\sum_{k=1}^K p_k (1 - p_k)$$\n",
    "\n",
    "* $p_k$ - доля объектов с классом $k$ среди $R$\n",
    "* $K$ - общее число классов\n",
    "\n",
    "При разбиении вершины на два поддерева мы хотим максимизировать функционал качества, оптимизируя:\n",
    "\n",
    "* $\\alpha$ - номер признака в предикате\n",
    "* $\\beta$ - пороговое значение предиката\n",
    "\n",
    "В данном задании оптимизацию мы будем проводить, используя полный перебор значений. Для $\\alpha$ множество перебираемых значений - все имеющиеся признаки, для $\\beta$ - все встречающиеся в обучающей выборке значения каждого признака, кроме наименьшего и наибольшего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Вычисление значения в листе дерева \n",
    "\n",
    "Начнём реализацию дерева с самого простого - реализация функции выдачи ответов в листе дерева. Будем в качестве такой функции использовать долю объектов каждого класса среди объектов обучающей выборки, попавших в лист.\n",
    "\n",
    "Ниже вам предлагается реализовать метод класса `ClassificationProbabilityMatcher` вычисляющий по поданному массиву ответов, долю каждого из классов по этому массиву. Предполагается, что массив y состоит из целых чисел от $0$ до $K-1$, где $K$ - количество классов.\n",
    "\n",
    "**Рекомендация.** Воспользуйтесь функций bincount из библиотеки numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePredictionMatcher:\n",
    "    def get_prediction_value(self, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ClassificationProbabilityMatcher(BasePredictionMatcher):\n",
    "    def __init__(self, n_classes=None):\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    def get_prediction_value(self, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy ndarray of shape (n_objects,) \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        classes_probabilities : numpy ndarray of shape (n_classes,)\n",
    "            classes_probabilities[i] - ith class probability\n",
    "        \"\"\"\n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте себя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_y = np.array([1, 1, 2, 4, 2, 2, 0, 1, 0, 4])\n",
    "probability_matcher = ClassificationProbabilityMatcher(n_classes=6)\n",
    "\n",
    "assert probability_matcher.get_prediction_value(some_y).tolist() == [0.2, 0.3, 0.3, 0, 0.2, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вычисление параметров предиката\n",
    "\n",
    "Ниже вам предлагается реализовать несколько методов. Для класса `BaseInformationCriterion` необходимо реализовать методы:\n",
    "* `find_best_split` - вычисление оптимального $\\beta$ и $Q$ по выбранному признаку и вектору ответов \n",
    "* `get_Q` - вычисление функционала $Q$\n",
    "\n",
    "Для класса `GeanyInformationCriterion` вам предлагается реализовать метод `get_H`, вычисляющий значение критерия информативности. \n",
    "\n",
    "**Рекомендация.** Реализовывайте методы в следующем порядке: `get_H`, `get_Q`, `find_best_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseInformationCriterion:\n",
    "    def find_best_split(self, X_feature, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_feature : numpy ndarray of shape (n_objects,)\n",
    "        \n",
    "        y : numpy ndarray of shape (n_objects,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        best_threshold : float \n",
    "        best_Q_value : float\n",
    "        \"\"\"\n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################\n",
    "        \n",
    "    def get_Q(self, H_main, y_left, y_right):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        H_main : float\n",
    "        \n",
    "        y_left : numpy ndarray of shape (n_objects_left,)\n",
    "        \n",
    "        y_right : numpy ndarray of shape (n_objects_right,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q_value : float\n",
    "        \"\"\"\n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################\n",
    "        \n",
    "    def get_H(self, y):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeanyInformationCriterion(BaseInformationCriterion):\n",
    "    def __init__(self, n_classes=None):\n",
    "        self.probability_matcher = ClassificationProbabilityMatcher(n_classes=n_classes)\n",
    "        \n",
    "    def get_H(self, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy ndarray of shape (n_objects,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        H_value : float\n",
    "        \"\"\"\n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте себя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geany_criterion = GeanyInformationCriterion(n_classes=5)\n",
    "\n",
    "\n",
    "main_H = geany_criterion.get_H(some_y)\n",
    "npt.assert_almost_equal(main_H, 0.74, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npt.assert_almost_equal(\n",
    "    geany_criterion.get_Q(main_H, [1, 1, 1, 0], [2, 2, 0, 2, 4, 4]),\n",
    "    0.22333333333333327,\n",
    "    decimal=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_X_feature = np.array([-5, -4, 1, 2, 3, 2, -3, -1, -2, 2])\n",
    "\n",
    "best_threshold, best_Q = geany_criterion.find_best_split(some_X_feature, some_y)\n",
    "assert best_threshold == 1\n",
    "npt.assert_almost_equal(best_Q, 0.26, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация вершины дерева\n",
    "\n",
    "Класс `TreeNode` задаёт одну вершину дерева (неважно, листовую или внутреннюю). У каждого объекта класса есть следующие атрибуты:\n",
    "\n",
    "* feature_index - индекс признака ($\\alpha$), по которому строится предикат\n",
    "* threshold - пороговое значение ($\\beta$), по которому строится предикат\n",
    "* prediction_value - значение, которое будет выдавать алгоритм для объектов, попавших в эту вершину, если она является листовой\n",
    "* right_children - ссылка на правую вершину-ребёнка\n",
    "* left_children - ссылка на левую веришну-ребёнка\n",
    "\n",
    "Вам необходимо реализовать два метода класса:\n",
    "* метод `get_next_nodes_mask` - получает значения предиката для попадающих в вершину объектов\n",
    "* метод `split_node` - превращает вершину из листовой в внутреннуюю, вычисляет по поданным X, y, критерию информативности и функции вычисления значений вершины значения параметров `feature_index` и `threshold` и создаёт две дочерних вершины, присваивая им нужные `prediction_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, prediction_value, feature_index=None, threshold=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.prediction_value = prediction_value\n",
    "        self.right_children = None\n",
    "        self.left_children = None\n",
    "        \n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        if self.right_children is None and self.left_children is None:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_next_nodes_mask(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray of shape (n_objects, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        next_node_indexes : float\n",
    "        \"\"\"\n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################\n",
    "        \n",
    "    def split_node(self, X, y, criterion, prediction_matcher):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray of shape (n_objects, n_features)\n",
    "        y : numpy ndarray of shape (n_objects,)\n",
    "        criterion : instance of BaseInformationCriterion inherited\n",
    "        prediction_matcher : instance of BasePredictionMatcher inherited\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        indexes_mask : numpy ndarray of shape (n_objects,)\n",
    "            Right children objects mask.\n",
    "        right_children : instance of TreeNode\n",
    "        left_children : instance of TreeNode\n",
    "        \"\"\"\n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_node = TreeNode(1, 0, 0.5)\n",
    "\n",
    "some_X = np.array([\n",
    "    [0, 1, 2],\n",
    "    [0.3, 1, 3],\n",
    "    [0.5, 1, 3],\n",
    "    [1, 2, 1]\n",
    "])\n",
    "\n",
    "some_y = np.array([0, 1, 1, 0])\n",
    "\n",
    "assert some_node.get_next_nodes_mask(some_X).tolist() == [False, False, True, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_node = TreeNode(1)\n",
    "geany_criterion = GeanyInformationCriterion(n_classes=2)\n",
    "\n",
    "mask, right_children, left_children = (\n",
    "    some_node.split_node(some_X, some_y, geany_criterion, geany_criterion.probability_matcher)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mask.tolist() == [False, True, True, False]\n",
    "assert right_children.prediction_value.tolist() == [0, 1]\n",
    "assert left_children.prediction_value.tolist() == [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна проверка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=2, min_leaf_size=1)\n",
    "\n",
    "some_X = np.vstack((\n",
    "    np.random.normal(loc=(-5, -5), size=(100, 2)),\n",
    "    np.random.normal(loc=(-5, 5), size=(100, 2)),\n",
    "    np.random.normal(loc=(5, -5), size=(100, 2)),\n",
    "    np.random.normal(loc=(5, 5), size=(100, 2)),\n",
    "))\n",
    "\n",
    "some_X = np.hstack((some_X, np.random.random((400, 100))))\n",
    "\n",
    "some_y = np.array(\n",
    "    [0] * 100 + [1] * 100 + [2] * 100 + [3] * 100\n",
    ")\n",
    "\n",
    "some_tree.fit(some_X, some_y)\n",
    "predictions = some_tree.predict(some_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "npt.assert_equal(predictions, some_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Реализация дерева.\n",
    "\n",
    "Вот мы и добрались до самого важного. В классе `DecisionTree` вам необходимо реализовать следующие методы:\n",
    "* fit - обучения дерева\n",
    "* predict - выдача предсказаний по дереву\n",
    "\n",
    "Для реализации предлагается использовать два вспомогательных метода: \n",
    "* _build_new_nodes - вспомогательный рекурсивный метод для fit, разделяет вершину на две, если не выполняются условия останова\n",
    "* _get_predictions_from_terminal_nodes - вспомогательный рекурсивный метод для predict, пропускает объекты через вершины и заполняет словарь предсказаний\n",
    "\n",
    "Важный вопрос при реализации: как выбрать критерий останова создания новой вершины? Вершина не должна разветвляться, если выполнено хотя бы одно из трёх условий:\n",
    "\n",
    "* если вершина на глубине max_depth\n",
    "* если в вершине меньше чем min_leaf_size объектов\n",
    "* если в вершине все объекты имеют одинаковые метки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self, criterion, prediction_matcher,\n",
    "        max_depth, min_leaf_size,\n",
    "    ):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        self.criterion = criterion\n",
    "        self.prediction_matcher = prediction_matcher\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray of shape (n_objects, n_features)\n",
    "        y : numpy ndarray of shape (n_objects,)\n",
    "        \"\"\"\n",
    "        base_answer = self.prediction_matcher.get_prediction_value(y)\n",
    "        self.root_node = TreeNode(prediction_value=base_answer)\n",
    "        \n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################\n",
    "        \n",
    "    def _build_new_nodes(self, X, y, current_objects_indexes, current_node, current_depth):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray of shape (n_objects, n_features)\n",
    "        y : numpy ndarray of shape (n_objects,)\n",
    "        current_objects_indexes : numpy ndarray of shape (n_objects_node,)\n",
    "            Indexes of current node objects\n",
    "        current_node : instance of TreeNode\n",
    "        current_depth : int\n",
    "        \"\"\"\n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray of shape (n_objects, n_features)\n",
    "        \"\"\"\n",
    "        predictions_dict = dict()\n",
    "        \n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################\n",
    "        \n",
    "        predictions = np.array([\n",
    "            elem[1] for elem in sorted(predictions_dict.items(), key=lambda x: x[0])\n",
    "        ])\n",
    "        return predictions\n",
    "    \n",
    "    def _get_predictions_from_terminal_nodes(self, X, current_objects_indexes, current_node,\n",
    "                                             current_predictions):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray of shape (n_objects, n_features)\n",
    "        current_objects_indexes : numpy ndarray of shape (n_objects_node,)\n",
    "        current_node : instance of TreeNode\n",
    "        current_predictions : dict\n",
    "        \"\"\"\n",
    "        #########################\n",
    "        ##  your code is here  ##\n",
    "        #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь на основе реализованного в общем виде дерева, сделаем дерево для классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDecisionTree(DecisionTree):\n",
    "    def __init__(self, max_depth, min_leaf_size):\n",
    "        criterion = GeanyInformationCriterion()\n",
    "        matcher = ClassificationProbabilityMatcher()\n",
    "        \n",
    "        super().__init__(\n",
    "            max_depth=max_depth,\n",
    "            min_leaf_size=min_leaf_size,\n",
    "            criterion=criterion,\n",
    "            prediction_matcher=matcher,\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_classes = y.max() + 1\n",
    "        self.prediction_matcher.n_classes = n_classes\n",
    "        \n",
    "        super().fit(X, y)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return super().predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = super().predict(X)\n",
    "        return probabilities.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверь себя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=2, min_leaf_size=1)\n",
    "\n",
    "some_X = np.vstack((\n",
    "    np.random.normal(loc=(-5, -5), size=(100, 2)),\n",
    "    np.random.normal(loc=(-5, 5), size=(100, 2)),\n",
    "    np.random.normal(loc=(5, -5), size=(100, 2)),\n",
    "    np.random.normal(loc=(5, 5), size=(100, 2)),\n",
    "))\n",
    "\n",
    "some_y = np.array(\n",
    "    [0] * 100 + [1] * 100 + [2] * 100 + [3] * 100\n",
    ")\n",
    "\n",
    "some_tree.fit(some_X, some_y)\n",
    "predictions = some_tree.predict(some_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "npt.assert_equal(predictions, some_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Визуализация результатов\n",
    "\n",
    "Давайте проверим, что дерево работает на нескольких модельных задачах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для визуализации двумерной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, figsize=(6, 5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    n_classes = y.max() + 1\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(X[:, 0][y == i], X[:, 1][y == i], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для визуализации работы дерева на двумерной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_surface(clf, X, y, plot_step=0.2, cmap='Spectral', figsize=(6, 5)):\n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    n_classes = len(set(y))\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)    \n",
    "    y_pred = clf.predict(X)\n",
    "\n",
    "    # Plot the training points\n",
    "    plt.scatter(*X[y_pred == y].T, marker='.', s=70,\n",
    "                c=y[y_pred == y], cmap=cmap, alpha=0.9, label='correct')\n",
    "    plt.scatter(*X[y_pred != y].T, marker='x', s=50,\n",
    "                c=y[y_pred != y], cmap=cmap, label='errors')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.axis(\"tight\")\n",
    "    plt.legend(loc='best')\n",
    "    print(\"Accuracy =\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала рассмотрим простую задачу с полностью разделимыми по некоторому признаку классами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, n_features=2, cluster_std=1.5, centers=2, random_state=23)\n",
    "\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим к этой задаче алгоритм \"решающий пень\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=1, min_leaf_size=1)\n",
    "some_tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы всё реализовали правильно, то алгоритм должен нарисовать горизонтальную разделяющую поверхность по нижней точки верхнего класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_surface(some_tree, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим более сложную задачу с плохо разделимыми классами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, n_features=2, cluster_std=2, centers=5, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты, которые получаются при разных значениях глубины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=1, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=2, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=3, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=5, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=10, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что начиная с некоторого значения глубины, дерево начинает сильно переобучаться (новые сплиты пытаются описать максимум 1-2 объекта). Это можно поправить, изменяя параметр min_leaf_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=10, min_leaf_size=10)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание (10 баллов)\n",
    "\n",
    "1. (4 балла) Доделать все пункты ноутбука до конца\n",
    "\n",
    "2. (2 балла) Для регрессионного дерева необходимо использовать такой критерий:\n",
    "    $$ H(R) = \\min_c \\frac{1}{|R|} \\sum_{(x_i, y_i) \\in R} (y_i - c)^2$$\n",
    "    \n",
    "    Докажите, что H(R) можно переписать в следующем виде:\n",
    "\n",
    "    $$ H(R) = \\frac{1}{|R|} \\left(y_i - \\frac{1}{|R|} \\sum_{(x_j, y_j) \\in R} y_j \\right)^2$$\n",
    "\n",
    "3. (2 балла) Реализуйте регрессионное дерево. В качестве критерия необходимо использовать критерий, определённый в пункте 2. В качестве функции выдачи результатов необходимо использовать среднее значение ответов по всем объектам в листе.\n",
    "\n",
    "    Сгенерируйте однопризнаковую выборку для тестирования дерева и покажите работу дерева на этой выборке. Отобразите на одном графике значения алгоритма и точки. Нарисуйте эту картинку для нескольких значений глубины. Сделайте выводы.\n",
    "    \n",
    "    \n",
    "3. (2 балла) Протестируйте ваше дерево на california_housing датасете (можно загрузить с помощью sklearn.datasets.fetch_california_housing).\n",
    "    Разбейте данные на обучение, контроль и тест. Подберите гиперпараметры по контрольной выборке, покажите качество алгоритма на тестовой. Сделайте выводы.\n",
    "\n",
    "\n",
    "Бонусных баллов в этот раз нет :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(R) = \\min_c \\frac{1}{|R|} \\sum_{(x_i, y_i) \\in R} (y_i - c)^2$$\n",
    "Тогда найдем минимум:\n",
    "$$ H(R)^{'}_c = \\frac{2}{|R|} \\sum_{(x_i, y_i) \\in R} (y_i - c) = 0 $$\n",
    "Откуда:\n",
    "$$ c = \\frac{1}{|R|} \\sum_{(x_i, y_i) \\in R} y_i $$ \n",
    "Подставим, получим:\n",
    "$$ H(R) = \\frac{1}{|R|} \\sum_{(x_i, y_i) \\in R}(y_i - \\frac{1}{|R|} \\sum_{(x_j, y_j) \\in R} y_j )^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
