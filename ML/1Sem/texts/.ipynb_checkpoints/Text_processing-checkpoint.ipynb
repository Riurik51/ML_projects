{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pymorphy2\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С приходом python 3 стало гораздо проще работать со строками, т.к. кодировка по умолчанию стала 'UTF-8'\n",
    "\n",
    "В python 2, при работе с неанглийскими строками, надо явно указывать кодировку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Он вернулся домой в Торбу-на-Круче 22 июня, на пятьдесят втором году жизни (в 1342 году по летоисчислению Удела).'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые полезные встроенные функции при работе со строками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пример\n",
      "lower: пример\n",
      "upper: ПРИМЕР\n",
      "capitalize: Пример\n"
     ]
    }
   ],
   "source": [
    "word = 'пример'\n",
    "print(word)\n",
    "print('lower: %s' % word.lower())\n",
    "print('upper: %s' % word.upper())\n",
    "print('capitalize: %s' % word.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tПредложение из нескольких слов.  \n",
      "strip: Предложение из нескольких слов.\n",
      "split: ['Предложение', 'из', 'нескольких', 'слов.']\n"
     ]
    }
   ],
   "source": [
    "sentence = '\\tПредложение из нескольких слов.  '\n",
    "print(sentence)\n",
    "print('strip: %s' % sentence.strip())\n",
    "sentence = sentence.strip()\n",
    "print('split: %s' % sentence.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регулярные выражения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как правило, нас не интересуют небуквенные символы (цифры, скобки и т.п)\n",
    "\n",
    "Самый простой и быстрый способ избавиться от ненужных символов - использование регулярных выражений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Он вернулся домой в ТорбунаКруче 22 июня на пятьдесят втором году жизни в 1342 году по летоисчислению Удела\n",
      "Он вернулся домой в ТорбунаКруче  июня на пятьдесят втором году жизни в  году по летоисчислению Удела\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('[^\\w\\s]', '', s))\n",
    "# \\w matches Unicode word characters; this includes most characters that can be part of a word in any language,\n",
    "# as well as numbers and the underscore\n",
    "\n",
    "print(re.sub('[^а-яёА-ЯЁ\\s]', '', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два метода приведения слова к нормальной форме:\n",
    "- лемматизация (\"начальная\" форма)\n",
    "- стеммирование (отбрасывание окончаний слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "ru_stemmer = nltk.stem.SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемматизация: летоисчисление\n",
      "Стеммирование: летоисчислен\n"
     ]
    }
   ],
   "source": [
    "print('Лемматизация: %s' % ru_lemmatizer.parse('летоисчислению')[0].normal_form)\n",
    "print('Стеммирование: %s' % ru_stemmer.stem('летоисчислению'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_lemmatizer.parse('стали')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pymorphy2 не использует контекста при нормализации (лемматизации слова), отсюда - несколько нормальных форм\n",
    "\n",
    "Score - условная вероятность $p(tag|word)$, собранная по корпусу OpenCorpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "стать\n"
     ]
    }
   ],
   "source": [
    "# самый \"правдоподобный разбор\"\n",
    "print(ru_lemmatizer.parse('стали')[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто, текст надо разделить на отдельные части (токены) - предложения или слова.\n",
    "\n",
    "Так сложилось, что чаще всего под токенами понимают именно слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ru_example.txt', 'r') as f:\n",
    "    ru_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Хоббиты - маленький народ, они меньше гномов: во всяком случае менее крепкие и приземистые, хотя ненамного меньше ростом. Их рост разнится от двух до четырех футов по нашим меркам. Теперь они редко достигают трех футов: но они утверждают, что становятся ниже и что в прошлые времена они были выше. В соответствии с \"Алой книгой\", Бандобрас Крол (по прозвищу Бычий Рык), сын Изенгрима Второго, был ростом в четыре фута пять дюймов, и мог ездить верхом на лошади. По преданием хоббитов его превосходят только два известных в древности хоббита, но об этом будет идти речь в этой книге\n"
     ]
    }
   ],
   "source": [
    "example_paragraph = ru_text[11062:11643]\n",
    "print(example_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проще всего пользоваться инструментами токенизации NLTK:\n",
    "- RegexpTokenizer\n",
    "- BlanklineTokenizer\n",
    "- и куча других"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Хоббиты - маленький народ, они меньше гномов: во всяком случае менее крепкие и приземистые, хотя ненамного меньше ростом.',\n",
       " 'Их рост разнится от двух до четырех футов по нашим меркам.',\n",
       " 'Теперь они редко достигают трех футов: но они утверждают, что становятся ниже и что в прошлые времена они были выше.',\n",
       " 'В соответствии с \"Алой книгой\", Бандобрас Крол (по прозвищу Бычий Рык), сын Изенгрима Второго, был ростом в четыре фута пять дюймов, и мог ездить верхом на лошади.',\n",
       " 'По преданием хоббитов его превосходят только два известных в древности хоббита, но об этом будет идти речь в этой книге']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# стандартный sent_tokenize разделяет предложения по знакам препинания (согласно правилам языка)\n",
    "nltk.sent_tokenize(example_paragraph, language='russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Хоббиты', '-', 'маленький', 'народ', ',', 'они', 'меньше', 'гномов', ':', 'во', 'всяком', 'случае', 'менее', 'крепкие', 'и', 'приземистые', ',', 'хотя', 'ненамного', 'меньше', 'ростом', '.', 'Их', 'рост', 'разнится', 'от', 'двух', 'до', 'четырех', 'футов', 'по', 'нашим', 'меркам', '.', 'Теперь', 'они', 'редко', 'достигают', 'трех', 'футов', ':', 'но', 'они', 'утверждают', ',', 'что', 'становятся', 'ниже', 'и', 'что', 'в', 'прошлые', 'времена', 'они', 'были', 'выше', '.', 'В', 'соответствии', 'с', '``', 'Алой', 'книгой', \"''\", ',', 'Бандобрас', 'Крол', '(', 'по', 'прозвищу', 'Бычий', 'Рык', ')', ',', 'сын', 'Изенгрима', 'Второго', ',', 'был', 'ростом', 'в', 'четыре', 'фута', 'пять', 'дюймов', ',', 'и', 'мог', 'ездить', 'верхом', 'на', 'лошади', '.', 'По', 'преданием', 'хоббитов', 'его', 'превосходят', 'только', 'два', 'известных', 'в', 'древности', 'хоббита', ',', 'но', 'об', 'этом', 'будет', 'идти', 'речь', 'в', 'этой', 'книге']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(example_paragraph, language='russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Один из возможных пайплайнов обработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub('[^а-яёa-z\\s]', '', sent)\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    lemma_words = [ru_lemmatizer.parse(word)[0].normal_form for word in sent_words]\n",
    "    processed_sent = ' '.join(lemma_words)\n",
    "    return processed_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = ru_text[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Со времени прибытия Гэндальфа он никому не показывался на глаза.\n",
      "с время прибытие гэндальф он никто не показываться на глаз\n",
      "\n",
      "Однажды утром хоббиты обнаружили, что большое поле к югу от входной двери Бильбо покрыто мотками веревки и столбами навесов и павильонов.\n",
      "однажды утром хоббит обнаружить что большой пол к юг от входной дверь бильбо покрыть мотка верёвка и столб навес и павильон\n",
      "\n",
      "В насыпи, выходящей на дорогу, был проделан особый проход, вырублены широкие ступени и построены большие белые ворота.\n",
      "в насыпь выходить на дорога быть проделать особый проход вырубить широкий ступень и построить большой белые ворот\n",
      "\n",
      "Три семейства хоббитов на Бэгшот-Роу, жившие по соседству с домом были особенно заинтересованы и чрезвычайно завидовали.\n",
      "три семейство хоббит на бэгшотроу жить по соседство с дом быть особенно заинтересовать и чрезвычайно завидовать\n",
      "\n",
      "Почтенный старик Скромби перестал даже делать вид, что работает в своем огороде.\n",
      "почтенный старик скромбить перестать даже делать вид что работать в свой огород\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(example_text)[500: 505]\n",
    "processed_sents = [preprocess_sentence(sent) for sent in sentences]\n",
    "\n",
    "for proc_sent, src_sent in zip(processed_sents, sentences):\n",
    "    print(src_sent)\n",
    "    print(proc_sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравним скорости работы стеммирования и лемматизации, а также лемматизации с кешированием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=int(10e5))\n",
    "\n",
    "def lru_lemmatize(word):\n",
    "    return ru_lemmatizer.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_part = ru_text[:100000]\n",
    "text_part = text_part.lower()\n",
    "text_part = re.sub('[^а-яёa-z\\s]', '', text_part)\n",
    "words = nltk.word_tokenize(text_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 813 ms, sys: 0 ns, total: 813 ms\n",
      "Wall time: 813 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stem_words = [ru_stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 366 ms, sys: 0 ns, total: 366 ms\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemm_words = [ru_lemmatizer.parse(word)[0].normal_form for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 0 ns, total: 122 ms\n",
      "Wall time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cache_lemm_words = [lru_lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Две \"количественные\" модели векторизации текста:\n",
    "- bag of words (мешок слов)\n",
    "- tf-idf\n",
    "\n",
    "Пусть есть корпус текстов:\n",
    "$$ D=\\{d_1, \\dots, d_{|D|}\\}, $$\n",
    "где каждый текст есть набор токенов (слов):\n",
    "$$ d_i = (w_1, \\dots, w_{n_i}), \\quad n_i - \\text{кол-во слов в тексте } d_i$$\n",
    "\n",
    "$tf(w, d)$ - term frequency, то, как часто слово $w$ встречалось в документе $d$.\n",
    "\n",
    "$idf(w)$ - inverted document frequency, то, насколько часто слово встречалось во всем корпусе документов\n",
    "\n",
    "Есть разные реализации, например:\n",
    "$$tf(w, d) = \\sum\\limits_{w'\\in d} \\mathbb{I}[w=w'] = n_{wd}$$\n",
    "$$tf(w, d) = \\mathbb{I}[w \\in d]$$\n",
    "$$tf(w, d) = \\frac{n_{wd}}{n_d}$$\n",
    "\n",
    "Для idf в sklearn используется:\n",
    "$$idf(w) = \\log \\left( \\frac{N}{\\sum_{i=1}^N \\mathbb{I}[w \\in d_i]}\\right) + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['он вернулся домой', 'Бильбо вернулся домой Бильбо', 'Бильбо оказался дома']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'он', 'Бильбо', 'дома', 'домой', 'вернулся', 'оказался'}\n"
     ]
    }
   ],
   "source": [
    "corpus_vocab = nltk.word_tokenize(' '.join(corpus))\n",
    "corpus_vocab = set(corpus_vocab)\n",
    "print(corpus_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1]\n",
      " [2 1 0 1 0 0]\n",
      " [1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer()\n",
    "print(count_vec.fit_transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.51785612 0.         0.51785612 0.         0.68091856]\n",
      " [0.81649658 0.40824829 0.         0.40824829 0.         0.        ]\n",
      " [0.4736296  0.         0.62276601 0.         0.62276601 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vec = TfidfVectorizer()\n",
    "print(tf_idf_vec.fit_transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
