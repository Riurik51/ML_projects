{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамбли\n",
    "\n",
    "### OzonMasters, \"Машинное обучение 1\"\n",
    "\n",
    "В этом ноутбуке вам предлагается реализовать алгоритмы бустинга и бэггинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Сэмплирование случайных объектов и признаков\n",
    "\n",
    "Во многих ансамблевых алгоритмах используется прием, заключающийся в обучении на случайной подвыборке объектов или на случайном подмножестве признаков.\n",
    "\n",
    "Так что для начала реализуем класс, который будет упрощать семплирование различных подмассивов данных\n",
    "\n",
    "В классе `ObjectSampler` надо реализовать метод `sample`, который возвращает случайную подвыборку объектов обучения и ответы для них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSampler:\n",
    "    def __init__(self, max_samples=1.0, bootstrap=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        bootstrap : Boolean\n",
    "            if True then use bootstrap sampling\n",
    "        max_samples : float in [0;1]\n",
    "            proportion of sampled examples\n",
    "        \"\"\"\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_samples = max_samples\n",
    "    \n",
    "    def sample(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ObjectSampler(BaseSampler):\n",
    "    def __init__(self, axis=0, max_samples=1.0, bootstrap=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        axis : int\n",
    "            which axis use to sample\n",
    "        \"\"\"\n",
    "        self.axis = axis\n",
    "        super().__init__(max_samples=max_samples, bootstrap=bootstrap)\n",
    "    \n",
    "    def sample(self, x, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy ndarray of shape (n_objects, n_features)\n",
    "        y : numpy ndarray of shape (n_objects,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x_sampled, y_sampled : numpy ndarrays of shape (n_samples, n_features) and (n_samples,)\n",
    "            n_samples = x_sampled.shape[0] * self.max_samples\n",
    "        \"\"\"\n",
    "        idx = np.random.choice(x.shape[0], int(x.shape[0] * self.max_samples), self.bootstrap)\n",
    "        x_sampled = x[idx]\n",
    "        y_sampled = y[idx]\n",
    "        return x_sampled, y_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В классе `FeaturesSampler` надо реализовать метод `sample`, который возвращает случайную подвыборку индексов признаков, по которым будет производится обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesSampler(BaseSampler):\n",
    "    def __init__(self, axis=1, max_samples=1.0, bootstrap=False):\n",
    "        self.axis = axis\n",
    "        super().__init__(max_samples=max_samples, bootstrap=bootstrap)\n",
    "        \n",
    "    def sample(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy ndarray of shape (n_objects, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        indices : numpy ndarrays of shape (n_features_sampled)\n",
    "        \"\"\"\n",
    "        \n",
    "        indices = np.random.choice(x.shape[1], int(x.shape[1] * self.max_samples), self.bootstrap)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_X = np.array([[0, 1, 2], [0.3, 1, 3], [0.5, 1, 3], [1, 2, 1]])\n",
    "some_y = np.array([1, 5, 3, 1])\n",
    "\n",
    "object_sampler = ObjectSampler(max_samples=0.7)\n",
    "feature_sampler = FeaturesSampler(max_samples=0.7)\n",
    "\n",
    "assert object_sampler.sample(some_X, some_y)[0].shape == (int(0.7*some_X.shape[0]), some_X.shape[1])\n",
    "assert object_sampler.sample(some_X, some_y)[1].shape == (int(0.7*some_y.shape[0]),)\n",
    "\n",
    "sample_X, sample_y = object_sampler.sample(some_X, some_y)\n",
    "\n",
    "for sub_x, sub_y in zip(sample_X, sample_y):\n",
    "    assert sub_x.tolist() in some_X.tolist()\n",
    "    assert sub_y in sample_y\n",
    "\n",
    "assert feature_sampler.sample(some_X).shape == (int(0.7 * some_X.shape[1]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Бэггинг\n",
    "\n",
    "Суть бэггинга заключается в обучении нескольких 'слабых' базовых моделей и объединении их в одну модель, обладающую бОльшей обобщающей способностью. Каждая базовая модель обучается на случайно выбранном подмножестве объектов и на случайно выбранном подмножестве признаков для этих объектов.\n",
    "\n",
    "Ниже вам предлагается реализовать несколько методов класса `Bagger`:\n",
    "* `fit` - обучение базовых моделей\n",
    "* `predict_proba` - вычисление вероятностей ответов\n",
    "* `predict` - вычисление ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagger:\n",
    "    def __init__(\n",
    "        self, base_estimator,\n",
    "        object_sampler, feature_sampler,\n",
    "        n_estimators=10, **params\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_estimators : int\n",
    "            number of base estimators\n",
    "        base_estimator : class\n",
    "            class for base_estimator with fit(), predict() and predict_proba() methods\n",
    "        feature_sampler : instance of FeaturesSampler\n",
    "        object_sampler : instance of ObjectSampler\n",
    "        estimators : list\n",
    "            list for containing base_estimator instances\n",
    "        indices : list\n",
    "            list for containing feature indices for each estimator\n",
    "        params : kwargs\n",
    "            params for base_estimator initialization\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.feature_sampler = feature_sampler\n",
    "        self.object_sampler = object_sampler\n",
    "        self.estimators = []\n",
    "        self.indices = []\n",
    "        self.params = params\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        for i in range(self.n_estimators):\n",
    "            1) select random indices of features for current estimator\n",
    "            2) select random objects and answers for train\n",
    "            3) fit base_estimator (don't forget to remain only selected features)\n",
    "            4) save base_estimator (self.estimators) and feature indices (self.indices)\n",
    "        \n",
    "        NOTE that self.base_estimator is class and you should init it with\n",
    "        self.base_estimator(**self.params) before fitting\n",
    "        \"\"\"\n",
    "        for i in range(self.n_estimators):\n",
    "            features = self.feature_sampler.sample(X)\n",
    "            X_samples, y_samples = self.object_sampler.sample(X, y)\n",
    "            estimator = self.base_estimator(**self.params)\n",
    "            estimator.fit(X_samples[:, features], y_samples)\n",
    "            self.estimators.append(estimator)\n",
    "            self.indices.append(features)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        probas : numpy ndarrays of shape (n_objects, n_classes)\n",
    "        \n",
    "        Calculate mean value of all probas from base_estimators\n",
    "        Don't forget, that each estimator has its own feature indices for prediction\n",
    "        \"\"\"\n",
    "        probas = self.estimators[0].predict_proba(X[:, self.indices[0]])\n",
    "        for i in range(1, self.n_estimators):\n",
    "            probas += self.estimators[i].predict_proba(X[:, self.indices[i]])\n",
    "        return np.array(probas) / self.n_estimators\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : numpy ndarrays of shape (n_objects, )\n",
    "        \n",
    "        \"\"\"\n",
    "        predictions = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, обучим бэггинг над решающими деревьями (случайный лес)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier(Bagger):\n",
    "    def __init__(self, n_estimators=30, max_depth=None, min_samples_leaf=1):\n",
    "        base_estimator = DecisionTreeClassifier\n",
    "        objects_sampler = ObjectSampler(max_samples=0.9)\n",
    "        features_sampler = FeaturesSampler(max_samples=0.8)\n",
    "        \n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            object_sampler=object_sampler,\n",
    "            feature_sampler=feature_sampler,\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_random_forest = RandomForestClassifier()\n",
    "\n",
    "some_X, some_y = make_classification(n_samples=30, n_features=50,\n",
    "                                     n_informative=50, n_redundant=0,\n",
    "                                     random_state=0, shuffle=False)\n",
    "\n",
    "some_random_forest.fit(some_X, some_y)\n",
    "predictions = some_random_forest.predict(some_X)\n",
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "npt.assert_equal(predictions, some_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Бустинг\n",
    "\n",
    "Бустинг последовательно обучает набор базовых моделей таким образом, что каждая следующая модель пытается исправить ошибки работы предыдущей модели. Логика того, как учитываются ошибки предыдущей модели может быть разной. В алгоритме градиентного бустинга каждая следующая модель обучается на \"невязках\" предыдущей модели, минимизируя итоговую функцию потерь. Причем вклад каждой следующей модели уменьшается с каждой итерацией. В данной реализации мы будем постоянно домножать вклад на коэффициент `lr`.\n",
    "\n",
    "То есть, при обучении очередной модели, ответ модифицируется следующим образом:\n",
    "$$y_{updated} = preds_{base} - lr * update,$$\n",
    "    где $preds_{base}$ - ответ предыдущей модели, $update$ - ответ текущей модели\n",
    "\n",
    "Ниже вам предлагается реализовать стратегию обучения базовых классификаторов для `GradientBoostingClassifier`:\n",
    "* `_fit_base_estimator` - обучение базовой модели\n",
    "* `_gradient` - расчет градиента функции ошибки\n",
    "В данном случае мы используем логистическую функцию потерь, градиент которой рассчитыватся как:\n",
    "$$\\nabla L(y, pred) = pred - y$$\n",
    "\n",
    "А также метод `predict` для класса `Booster`. Учитывая, что каждая модель улучшает предсказания предыдущей, то предсказания обновляются как: `preds = preds - lr * estimator[i].predict`\n",
    "\n",
    "Так как мы работаем с потерями функции потерь для классификации и вероятностями классификации, то метки классов приводятся к one-hot виду. В качестве базовых моделей используются модели регрессии, чтобы предсказывать остаток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Booster:\n",
    "    def __init__(\n",
    "        self, base_estimator, feature_sampler,\n",
    "        n_estimators=10, lr=.5, **params\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_estimators : int\n",
    "            number of base estimators\n",
    "        base_estimator : class\n",
    "            class for base_estimator with fit(), predict() and predict_proba() methods\n",
    "        feature_sampler : instance of FeaturesSampler\n",
    "        estimators : list\n",
    "            list for containing base_estimator instances\n",
    "        indices : list\n",
    "            list for containing feature indices for each estimator\n",
    "        lr : float\n",
    "            learning rate for estimators\n",
    "        params : kwargs\n",
    "            kwargs for base_estimator init\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.feature_sampler = feature_sampler\n",
    "        self.estimators = []\n",
    "        self.indices = []\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "    \n",
    "    def _to_categorical(self, x, n_col=None):\n",
    "        \"\"\" \n",
    "        One-hot encoding of nominal values\n",
    "        \"\"\"\n",
    "        if not n_col:\n",
    "            n_col = np.amax(x) + 1\n",
    "        one_hot = np.zeros((x.shape[0], n_col))\n",
    "        one_hot[np.arange(x.shape[0]), x] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def _fit_first_estimator(self, X, y):\n",
    "        \"\"\"\n",
    "        fits first model and saves it to self.estimators\n",
    "        (and its indicies to self.indices)\n",
    "        \"\"\"\n",
    "        feature_indices = self.feature_sampler.sample(X)\n",
    "        self.indices.append(feature_indices)\n",
    "        curr_estimator = self.base_estimator(**self.params).fit(np.take(X, feature_indices, axis=1), y)\n",
    "        self.estimators.append(curr_estimator)\n",
    "        \n",
    "    def _fit_base_estimator(self, X, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        iteratively fits base models\n",
    "        \"\"\"\n",
    "        y = self._to_categorical(y)\n",
    "        self._fit_first_estimator(X, y)\n",
    "        predictions_base = self.estimators[-1].predict(np.take(X, self.indices[-1], axis=1))\n",
    "        for i in range(self.n_estimators - 1):\n",
    "            predictions_base = self._fit_base_estimator(X, y, predictions_base)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : numpy ndarrays of shape (n_objects, n_classes)\n",
    "        -------\n",
    "        1) get predictions by first model (self.estimators[0])\n",
    "        2) for each estimator in self.estimators[1:] (and its indicies):\n",
    "            update predictions\n",
    "        3) turn into probability distribution as\n",
    "        probas = np.exp(predictions) / np.expand_dims(np.sum(np.exp(predictions), axis=1), axis=1)\n",
    "        \"\"\"\n",
    "        predictions = self.estimators[0].predict(X[:, self.indices[0]])\n",
    "        for estimator, idx in zip(self.estimators[1:], self.indices[1:]):\n",
    "            predictions -= self.lr * estimator.predict(X[:, idx])\n",
    "        return np.exp(predictions) / np.expand_dims(np.sum(np.exp(predictions), axis=1), axis=1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return np.argmax(probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GradientBoostingClassifier(Booster):\n",
    "    \n",
    "    def __init__(self, n_estimators=30, lr=0.5, max_depth=None, min_samples_leaf=1):\n",
    "        \"\"\"\n",
    "        n_estimators : int\n",
    "            number of base estimators\n",
    "        base_estimator : class\n",
    "            class for base_estimator with fit(), predict() and predict_proba() methods\n",
    "        feature_sampler : instance of FeaturesSampler\n",
    "        estimators : list\n",
    "            list for containing base_estimator instances\n",
    "        indices : list\n",
    "            list for containing feature indices for each estimator\n",
    "        lr : float\n",
    "            learning rate for estimators\n",
    "        params : kwargs\n",
    "            kwargs for base_estimator init\n",
    "        \"\"\"\n",
    "        \n",
    "        base_estimator = DecisionTreeRegressor\n",
    "        feature_sampler = FeaturesSampler(max_samples=0.8)\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            feature_sampler=feature_sampler,\n",
    "            n_estimators=n_estimators,\n",
    "            lr=lr,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf\n",
    "        )\n",
    "        \n",
    "    def _gradient(self, y, pred):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : numpy ndarrays of shape (n_objects, n_classes)\n",
    "        \"\"\"\n",
    "        gradient = pred - y\n",
    "        return gradient\n",
    "    \n",
    "    def _fit_base_estimator(self, X, y, predictions_base):\n",
    "        \"\"\"\n",
    "        X : numpy ndarrays of shape (n_objects, n_features)\n",
    "        y : numpy ndarrays of shape (n_objects, )\n",
    "        predictions_base : numpy ndarrays of shape (n_objects, n_classes)\n",
    "            updated predictions from previous step\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        y_updated : numpy ndarrays of shape (n_objects, n_classes)\n",
    "            updated predictions\n",
    "        -------\n",
    "        \n",
    "        1) calculate gradient\n",
    "        2) select random indices of features for current estimator\n",
    "        3) fit estimator with predictions_base as target\n",
    "        4) calculate y_updated\n",
    "        5) save estimator and indicies\n",
    "        \n",
    "        NOTE that self.base_estimator is class and you should init it with\n",
    "        self.base_estimator(**self.params) before fitting\n",
    "        \"\"\"\n",
    "        grad = self._gradient(y, predictions_base)\n",
    "        features = self.feature_sampler.sample(X)\n",
    "        estimator = self.base_estimator(**self.params).fit(X[:, features], grad)\n",
    "        y_updated = predictions_base - self.lr * estimator.predict(X[:, features])\n",
    "        self.estimators.append(estimator)\n",
    "        self.indices.append(features)\n",
    "        return y_updated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_gradient_classifier = GradientBoostingClassifier()\n",
    "some_gradient_classifier.fit(some_X, some_y)\n",
    "predictions = some_gradient_classifier.predict(some_X)\n",
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "npt.assert_equal(predictions, some_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
