{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамбли\n",
    "\n",
    "### OzonMasters, \"Машинное обучение 1\"\n",
    "\n",
    "В этом ноутбуке вам предлагается реализовать алгоритмы бустинга и бэггинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Сэмплирование случайных объектов и признаков\n",
    "\n",
    "Во многих ансамблевых алгоритмах используется прием, заключающийся в обучении на случайной подвыборке объектов или на случайном подмножестве признаков.\n",
    "\n",
    "Так что для начала реализуем класс, который будет упрощать семплирование различных подмассивов данных\n",
    "\n",
    "В классе `ObjectSampler` надо реализовать метод `sample`, который возвращает случайную подвыборку объектов обучения и ответы для них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSampler:\n",
    "    def __init__(self, max_samples=1.0, bootstrap=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        bootstrap : Boolean\n",
    "            if True then use bootstrap sampling\n",
    "        max_samples : float in [0;1]\n",
    "            proportion of sampled examples\n",
    "        \"\"\"\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_samples = max_samples\n",
    "    \n",
    "    def sample(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ObjectSampler(BaseSampler):\n",
    "    def __init__(self, axis=0, max_samples=1.0, bootstrap=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        axis : int\n",
    "            which axis use to sample\n",
    "        \"\"\"\n",
    "        self.axis = axis\n",
    "        super().__init__(max_samples=max_samples, bootstrap=bootstrap)\n",
    "    \n",
    "    def sample(self, x, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy ndarray of shape (n_objects, n_features)\n",
    "        y : numpy ndarray of shape (n_objects,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x_sampled, y_sampled : numpy ndarrays of shape (n_samples, n_features) and (n_samples,)\n",
    "            n_samples = x_sampled.shape[0] * self.max_samples\n",
    "        \"\"\"\n",
    "        idx = np.random.choice(x.shape[0], int(x.shape[0] * self.max_samples), self.bootstrap)\n",
    "        x_sampled = x[idx]\n",
    "        y_sampled = y[idx]\n",
    "        return x_sampled, y_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В классе `FeaturesSampler` надо реализовать метод `sample`, который возвращает случайную подвыборку индексов признаков, по которым будет производится обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesSampler(BaseSampler):\n",
    "    def __init__(self, axis=1, max_samples=1.0, bootstrap=False):\n",
    "        self.axis = axis\n",
    "        super().__init__(max_samples=max_samples, bootstrap=bootstrap)\n",
    "        \n",
    "    def sample(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy ndarray of shape (n_objects, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        indices : numpy ndarrays of shape (n_features_sampled)\n",
    "        \"\"\"\n",
    "        indices = np.random.choice(x.shape[1], int(x.shape[1] * self.max_samples), self.bootstrap)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_X = np.array([[0, 1, 2], [0.3, 1, 3], [0.5, 1, 3], [1, 2, 1]])\n",
    "some_y = np.array([1, 5, 3, 1])\n",
    "\n",
    "object_sampler = ObjectSampler(max_samples=0.7)\n",
    "feature_sampler = FeaturesSampler(max_samples=0.7)\n",
    "\n",
    "assert object_sampler.sample(some_X, some_y)[0].shape == (int(0.7*some_X.shape[0]), some_X.shape[1])\n",
    "assert object_sampler.sample(some_X, some_y)[1].shape == (int(0.7*some_y.shape[0]),)\n",
    "\n",
    "sample_X, sample_y = object_sampler.sample(some_X, some_y)\n",
    "\n",
    "for sub_x, sub_y in zip(sample_X, sample_y):\n",
    "    assert sub_x.tolist() in some_X.tolist()\n",
    "    assert sub_y in sample_y\n",
    "\n",
    "assert feature_sampler.sample(some_X).shape == (int(0.7*some_X.shape[1]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Бэггинг\n",
    "\n",
    "Суть бэггинга заключается в обучении нескольких 'слабых' базовых моделей и объединении их в одну модель, обладающую бОльшей обобщающей способностью. Каждая базовая модель обучается на случайно выбранном подмножестве объектов и на случайно выбранном подмножестве признаков для этих объектов.\n",
    "\n",
    "Ниже вам предлагается реализовать несколько методов класса `Bagger`:\n",
    "* `fit` - обучение базовых моделей\n",
    "* `predict_proba` - вычисление вероятностей ответов\n",
    "* `predict` - вычисление ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagger:\n",
    "    def __init__(\n",
    "        self, base_estimator,\n",
    "        object_sampler, feature_sampler,\n",
    "        n_estimators=10, **params\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_estimators : int\n",
    "            number of base estimators\n",
    "        base_estimator : class\n",
    "            class for base_estimator with fit(), predict() and predict_proba() methods\n",
    "        feature_sampler : instance of FeaturesSampler\n",
    "        object_sampler : instance of ObjectSampler\n",
    "        estimators : list\n",
    "            list for containing base_estimator instances\n",
    "        indices : list\n",
    "            list for containing feature indices for each estimator\n",
    "        params : kwargs\n",
    "            params for base_estimator initialization\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.feature_sampler = feature_sampler\n",
    "        self.object_sampler = object_sampler\n",
    "        self.estimators = []\n",
    "        self.indices = []\n",
    "        self.params = params\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        for i in range(self.n_estimators):\n",
    "            1) select random indices of features for current estimator\n",
    "            2) select random objects and answers for train\n",
    "            3) fit base_estimator (don't forget to remain only selected features)\n",
    "            4) save base_estimator (self.estimators) and feature indices (self.indices)\n",
    "        \n",
    "        NOTE that self.base_estimator is class and you should init it with\n",
    "        self.base_estimator(**self.params) before fitting\n",
    "        \"\"\"\n",
    "        for i in range(self.n_estimators):\n",
    "            features = self.feature_sampler.sample(X)\n",
    "            X_samples, y_samples = self.object_sampler.sample(X, y)\n",
    "            estimator = self.base_estimator(**self.params)\n",
    "            estimator.fit(X_samples[:, features], y_samples)\n",
    "            self.estimators.append(estimator)\n",
    "            self.indices.append(features)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        probas : numpy ndarrays of shape (n_objects, n_classes)\n",
    "        \n",
    "        Calculate mean value of all probas from base_estimators\n",
    "        Don't forget, that each estimator has its own feature indices for prediction\n",
    "        \"\"\"\n",
    "        probas = self.estimators[0].predict_proba(X[:, self.indices[0]])\n",
    "        for i in range(1, self.n_estimators):\n",
    "            probas += self.estimators[i].predict_proba(X[:, self.indices[i]])\n",
    "        return np.array(probas) / self.n_estimators\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : numpy ndarrays of shape (n_objects, )\n",
    "        \n",
    "        \"\"\"\n",
    "        predictions = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, обучим бэггинг над решающими деревьями (случайный лес)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier(Bagger):\n",
    "    def __init__(self, n_estimators=30, max_depth=None, min_samples_leaf=1):\n",
    "        base_estimator = DecisionTreeClassifier\n",
    "        objects_sampler = ObjectSampler(max_samples=0.9)\n",
    "        features_sampler = FeaturesSampler(max_samples=0.8)\n",
    "        \n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            object_sampler=object_sampler,\n",
    "            feature_sampler=feature_sampler,\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_random_forest = RandomForestClassifier()\n",
    "\n",
    "some_X, some_y = make_classification(n_samples=30, n_features=50,\n",
    "                                     n_informative=50, n_redundant=0,\n",
    "                                     random_state=0, shuffle=False)\n",
    "\n",
    "some_random_forest.fit(some_X, some_y)\n",
    "predictions = some_random_forest.predict(some_X)\n",
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "npt.assert_equal(predictions, some_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Градиентный бустинг\n",
    "\n",
    "Бустинг последовательно обучает набор базовых моделей таким образом, что каждая следующая модель пытается исправить ошибки работы предыдущей модели. Логика того, как учитываются ошибки предыдущей модели может быть разной. В алгоритме градиентного бустинга каждая следующая модель обучается на \"невязках\" предыдущей модели, минимизируя итоговую функцию потерь. У каждого следующего алгоритма вычисляется вес $\\alpha$, с которым он входит в ансамбль. Также есть параметр скорости обучения (learning rate), который не позволяет алгоритму переобучитсья. Вес $\\alpha$ можно находить, используя одномерную оптимизацию. Можно записать процедуру обучения по шагам (будем рассматривать случай бинарной классификации c метками классов {0,1}, чтобы не усложнять жизнь):\n",
    "1. Настройка базового алгоритма $b_0$.\n",
    "    \n",
    "    Алгоритм настраиваются на $y$ с помощью функции MSE.\n",
    "    \n",
    "    \n",
    "2. Будем обозначать текущий небазовый алгоритм - $a$:\n",
    "    \n",
    "    $$a_i(x) = \\sum_{j=0}^i \\alpha_j b_j(x) $$\n",
    "    \n",
    "3. Настройка базового алгоритма $b_i$ (обычно это регрессионное дерево):\n",
    "    \n",
    "    $$b_i = \\arg \\min_b \\sum_{j=1}^l (b(x_j) + \\nabla L(a_{i-1}(x_j), y))^2,$$\n",
    "    т.е. выход очередного базового алгоритма подстраивается под антиградиент функции потерь\n",
    "    \n",
    "4. Настройка веса базового алгоритма $\\alpha_i$:\n",
    "    \n",
    "    $$\\alpha_i = \\min_{\\alpha > 0} \\sum_{j=1}^l L(a_{i-1} + \\alpha b_i(x_j), y) $$\n",
    "    \n",
    "В случае классфикации будем использовать логистическую функцию потерь. Немного упростим ее:\n",
    "\n",
    "$$L = -y\\log\\sigma(a) - (1-y)\\log(1 - \\sigma(a)) = -\\log(1 - \\sigma(a)) - y \\log \\frac{\\sigma(a)}{1 - \\sigma(a)},$$\n",
    "где $\\sigma$ - функция сигмоиды. Ответ после очередного базового алгоритма надо прогонять через сигмоиду, т.к. не гарантируется, что ответы будут лежать на [0,1] - в этом особенность базового алгоритма (который является регрессионным).\n",
    "\n",
    "Преобразуем:\n",
    "$$\\log (1 - \\sigma(a)) = \\log \\frac{1}{1 + \\exp(a)} = -\\log(1 + \\exp(a)) $$\n",
    "\n",
    "$$\\log (\\frac{\\sigma(a)}{1 - \\sigma(a)}) = \\log(\\exp(a)) = a $$\n",
    " \n",
    "Таким образом:\n",
    "\n",
    "$$L = -ya + \\log(1 + \\exp(a))$$\n",
    "\n",
    "Тогда будем вычислять градиент как:\n",
    " \n",
    "$$\\nabla L = - y + \\sigma(a)$$\n",
    "\n",
    "Ниже вам предлагается реализовать стратегию обучения базовых классификаторов для `GradientBoostingClassifier`:\n",
    "* `_fit_first_estimator` - обучение первой базовой модели\n",
    "* `_fit_base_estimator` - обучение базовой модели\n",
    "* `log_loss_gradient` - расчет градиента функции ошибки\n",
    "* `log_loss` - расчет функции ошибки (для одномерной оптимизации)\n",
    "\n",
    "А также метод `predict_proba` для класса `BoosterClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy.optimize import shgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoosterClassifier:\n",
    "    last_indices = []\n",
    "    last_estimators = []\n",
    "    last_weights = []\n",
    "\n",
    "    def __init__(\n",
    "        self, base_estimator, feature_sampler,\n",
    "        n_estimators=10, warm_start=False, lr=.5, **params\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_estimators : int\n",
    "            number of base estimators\n",
    "        base_estimator : class\n",
    "            class for base_estimator with fit(), predict() and predict_proba() methods\n",
    "        feature_sampler : instance of FeaturesSampler\n",
    "        estimators : list\n",
    "            list for containing base_estimator instances\n",
    "        indices : list\n",
    "            list for containing feature indices for each estimator\n",
    "        weights : list\n",
    "            list for containing estimators weights\n",
    "        lr : float\n",
    "            learning rate for estimators\n",
    "        params : kwargs\n",
    "            kwargs for base_estimator init\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.feature_sampler = feature_sampler\n",
    "        self.estimators = []\n",
    "        self.indices = []\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "        self.weights = []\n",
    "        self.warm_start = warm_start\n",
    "\n",
    "    @classmethod\n",
    "    def update_all(cls, indices, estimators, weights):\n",
    "        cls.last_indices = indices\n",
    "        cls.last_estimators = estimators\n",
    "        cls.last_weights = weights\n",
    "    \n",
    "    def _fit_first_estimator(self, X, y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _fit_base_estimator(self, X, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X : X : numpy ndarray of shape (n_objects, n_features)\n",
    "        y : numpy ndarray of shape (n_objects, )\n",
    "        -------\n",
    "        iteratively fits base models\n",
    "        \"\"\"\n",
    "        if self.warm_start is False or len(self.last_indices) == 0:\n",
    "            self._fit_first_estimator(X, y)\n",
    "            predictions_base = self.estimators[-1].predict(np.take(X, self.indices[-1], axis=1)) * self.weights[0]\n",
    "            for i in range(self.n_estimators - 1):\n",
    "                predictions_base = self._fit_base_estimator(X, y, predictions_base)\n",
    "        else:\n",
    "            self.estimators = self.last_estimators\n",
    "            self.indices = self.last_indices\n",
    "            self.weights = self.last_weights\n",
    "            predictions_base = self.estimators[0].predict(X[:, self.indices[0]]) * self.weights[0]\n",
    "            k = len(self.estimators)\n",
    "            for i in range(1, k):\n",
    "                predictions_base += self.estimators[i].predict(X[:, self.indices[i]]) * self.weights[i]\n",
    "            for i in range(k, self.n_estimators):\n",
    "                predictions_base = self._fit_base_estimator(X, y, predictions_base)\n",
    "        self.update_all(self.indices, self.estimators, self.weights)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        probas : numpy ndarray of shape (n_objects, )\n",
    "            predictions for one class\n",
    "        -------\n",
    "        1) get predictions by first model (self.estimators[0])\n",
    "        2) for each estimator in self.estimators[1:] (and its indicies and weights):\n",
    "            update predictions\n",
    "        3) turn into probability distribution with sigmoid function\n",
    "        \"\"\"\n",
    "        probas = self.estimators[0].predict(X[:, self.indices[0]]) * self.weights[0]\n",
    "        for i in range(1, self.n_estimators):\n",
    "            probas += self.estimators[i].predict(X[:, self.indices[i]]) * self.weights[i]\n",
    "        probas = expit(probas)\n",
    "        return probas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier(BoosterClassifier):\n",
    "    def __init__(self, n_estimators=30, lr=0.5, max_depth=2, min_samples_leaf=1, warm_start=False):\n",
    "        \"\"\"\n",
    "        n_estimators : int\n",
    "            number of base estimators\n",
    "        base_estimator : class\n",
    "            class for base_estimator with fit(), predict() and predict_proba() methods\n",
    "        feature_sampler : instance of FeaturesSampler\n",
    "        estimators : list\n",
    "            list for containing base_estimator instances\n",
    "        indices : list\n",
    "            list for containing feature indices for each estimator\n",
    "        lr : float\n",
    "            learning rate for estimators\n",
    "        params : kwargs\n",
    "            kwargs for base_estimator init\n",
    "        \"\"\"\n",
    "        \n",
    "        base_estimator = DecisionTreeRegressor\n",
    "        feature_sampler = FeaturesSampler(max_samples=0.8)\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            feature_sampler=feature_sampler,\n",
    "            n_estimators=n_estimators,\n",
    "            lr=lr,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            warm_start=warm_start\n",
    "        )\n",
    "\n",
    "    def log_loss_gradient(self, y, pred):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : numpy ndarrays of shape (n_objects, )\n",
    "        \"\"\"\n",
    "        gradient = y - expit(pred)\n",
    "        return gradient\n",
    "    \n",
    "    def log_loss(self, y, pred):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        loss : log loss for inputs\n",
    "        \"\"\"\n",
    "        loss = np.logaddexp(0, pred) - (y * pred)\n",
    "        return loss\n",
    "    \n",
    "    def function_to_optimize(self, y, prediction_pred, prediction_est, alpha):\n",
    "        return self.log_loss(y, prediction_pred + alpha * prediction_est).sum()\n",
    "    \n",
    "    def _fit_first_estimator(self, X, y):\n",
    "        \"\"\"\n",
    "        1) select random indices of features for first estimator\n",
    "        2) fit base_estimator (don't forget to remain only selected features)\n",
    "        3) calculate optimal weight for estimator by optimization\n",
    "        4) save base_estimator (self.estimators), feature indices (self.indices) and weight (self.weights)\n",
    "        \n",
    "        NOTE that self.base_estimator is class and you should init it with\n",
    "        self.base_estimator(**self.params) before fitting\n",
    "        \"\"\"\n",
    "        feature_indices = self.feature_sampler.sample(X)\n",
    "        self.indices.append(feature_indices)\n",
    "        first_estimator = self.base_estimator(**self.params).fit(X[:, feature_indices], y)\n",
    "        self.estimators.append(first_estimator)\n",
    "        prediction = first_estimator.predict(X[:, feature_indices])\n",
    "        alpha = shgo(self.function_to_optimize,\n",
    "                      [(0.0, 1.0)],\n",
    "                      (y, 0, prediction)\n",
    "                     ).x\n",
    "        self.weights.append(alpha)\n",
    "    \n",
    "    def _fit_base_estimator(self, X, y, predictions_base):\n",
    "        \"\"\"\n",
    "        X : numpy ndarrays of shape (n_objects, n_features)\n",
    "        y : numpy ndarrays of shape (n_objects, )\n",
    "        predictions_base : numpy ndarrays of shape (n_objects, n_classes)\n",
    "            updated predictions from previous step\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        y_updated : numpy ndarrays of shape (n_objects, n_classes)\n",
    "            updated predictions\n",
    "        -------\n",
    "        \n",
    "        1) calculate gradient\n",
    "        2) select random indices of features for current estimator\n",
    "        3) fit estimator with predictions_base as target\n",
    "        4) calculate optimal weight for estimator by optimization\n",
    "        5) calculate y_updated\n",
    "        6) save estimator, indicies and weights\n",
    "        \n",
    "        NOTE that self.base_estimator is class and you should init it with\n",
    "        self.base_estimator(**self.params) before fitting\n",
    "        \"\"\"\n",
    "        grad = self.log_loss_gradient(y, predictions_base)\n",
    "        feature_indices = self.feature_sampler.sample(X)\n",
    "        estimator = self.base_estimator(**self.params).fit(X[:, feature_indices], grad)\n",
    "        prediction = estimator.predict(X[:, feature_indices])\n",
    "        alpha = shgo(self.function_to_optimize,\n",
    "                      [(0.0, 1.0)],\n",
    "                      (y, predictions_base, prediction)\n",
    "                     ).x\n",
    "        y_updated = predictions_base + alpha * prediction\n",
    "        self.indices.append(feature_indices)\n",
    "        self.estimators.append(estimator)\n",
    "        self.weights.append(alpha)\n",
    "        return y_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_gradient_classifier = GradientBoostingClassifier()\n",
    "some_gradient_classifier.fit(some_X, some_y)\n",
    "predictions = some_gradient_classifier.predict(some_X)\n",
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "npt.assert_equal(predictions, some_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты\n",
    "\n",
    "Скачайте датасейт для экспериментов: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
    "\n",
    "Колонка с ответами - RainTommorow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделите признаки год/месяц/день:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['Date'].map(lambda x: x.year)\n",
    "data['month'] = data['Date'].map(lambda x: x.month)\n",
    "data['day'] = data['Date'].map(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие года есть в выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016    17508\n",
       "2014    17400\n",
       "2015    17231\n",
       "2009    16595\n",
       "2010    16419\n",
       "2013    16097\n",
       "2011    15126\n",
       "2012    15044\n",
       "2017     8466\n",
       "2008     2246\n",
       "2007       61\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на три части (train, val и test) по временному принципу:\n",
    "    \n",
    "* train - 2007-2014\n",
    "* val - 2015\n",
    "* test - 2016-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['year'].astype(int)\n",
    "\n",
    "indexes = {\n",
    "    'train': np.array(data.loc[data['year'] < 2015].index),\n",
    "    'val': np.array(data.loc[data['year'] == 2015].index),\n",
    "    'test': np.array(data.loc[data['year'] > 2015].index)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь вы можете делать всевозможные преобразования признаков. \n",
    "\n",
    "Для того, чтобы получить качество, необходимое для преодоления бейзлайна, вам достаточно закодировать все категориальные признаки с помощью LabelEncoder, а также разумно обработать пропущенные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваш таргет - RainTommorow. Удалите его из обучающих данных, также удалите признак RISK_MM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = data['RainTomorrow']\n",
    "data.drop(['RainTomorrow', 'RISK_MM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого из алгоритмов достигнутое качество должно быть: \n",
    "* RandomForest > 0.84\n",
    "* GradientBoosting > 0.845\n",
    "\n",
    "Для алгоритма AdaBoost, который предлагается реализовать в бонусной части, качество должно быть:\n",
    "* AdaBoost > 0.83\n",
    "\n",
    "Обучите каждый из алгоритмов до нужного качества, используйте валидационную выборку, чтобы подбирать гиперпараметры. Получите качество (accuracy) выше необходимого и на validation, и на test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>WNW</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>WSW</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>WSW</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>NW</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142188</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>3.5</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>31.0</td>\n",
       "      <td>ESE</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>1024.7</td>\n",
       "      <td>1021.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.4</td>\n",
       "      <td>20.9</td>\n",
       "      <td>No</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142189</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>2.8</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>31.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>1024.6</td>\n",
       "      <td>1020.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1</td>\n",
       "      <td>22.4</td>\n",
       "      <td>No</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142190</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>3.6</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNW</td>\n",
       "      <td>22.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>1019.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.5</td>\n",
       "      <td>No</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142191</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>5.4</td>\n",
       "      <td>26.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>37.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>WNW</td>\n",
       "      <td>...</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>1016.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>26.1</td>\n",
       "      <td>No</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142192</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>7.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SE</td>\n",
       "      <td>28.0</td>\n",
       "      <td>SSE</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>1019.4</td>\n",
       "      <td>1016.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142193 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0        Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1        Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2        Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3        Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4        Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "...         ...      ...      ...       ...          ...       ...   \n",
       "142188    Uluru      3.5     21.8       0.0          NaN       NaN   \n",
       "142189    Uluru      2.8     23.4       0.0          NaN       NaN   \n",
       "142190    Uluru      3.6     25.3       0.0          NaN       NaN   \n",
       "142191    Uluru      5.4     26.9       0.0          NaN       NaN   \n",
       "142192    Uluru      7.8     27.0       0.0          NaN       NaN   \n",
       "\n",
       "       WindGustDir  WindGustSpeed WindDir9am WindDir3pm  ...  Pressure9am  \\\n",
       "0                W           44.0          W        WNW  ...       1007.7   \n",
       "1              WNW           44.0        NNW        WSW  ...       1010.6   \n",
       "2              WSW           46.0          W        WSW  ...       1007.6   \n",
       "3               NE           24.0         SE          E  ...       1017.6   \n",
       "4                W           41.0        ENE         NW  ...       1010.8   \n",
       "...            ...            ...        ...        ...  ...          ...   \n",
       "142188           E           31.0        ESE          E  ...       1024.7   \n",
       "142189           E           31.0         SE        ENE  ...       1024.6   \n",
       "142190         NNW           22.0         SE          N  ...       1023.5   \n",
       "142191           N           37.0         SE        WNW  ...       1021.0   \n",
       "142192          SE           28.0        SSE          N  ...       1019.4   \n",
       "\n",
       "        Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  year  \\\n",
       "0            1007.1       8.0       NaN     16.9     21.8         No  2008   \n",
       "1            1007.8       NaN       NaN     17.2     24.3         No  2008   \n",
       "2            1008.7       NaN       2.0     21.0     23.2         No  2008   \n",
       "3            1012.8       NaN       NaN     18.1     26.5         No  2008   \n",
       "4            1006.0       7.0       8.0     17.8     29.7         No  2008   \n",
       "...             ...       ...       ...      ...      ...        ...   ...   \n",
       "142188       1021.2       NaN       NaN      9.4     20.9         No  2017   \n",
       "142189       1020.3       NaN       NaN     10.1     22.4         No  2017   \n",
       "142190       1019.1       NaN       NaN     10.9     24.5         No  2017   \n",
       "142191       1016.8       NaN       NaN     12.5     26.1         No  2017   \n",
       "142192       1016.5       3.0       2.0     15.1     26.0         No  2017   \n",
       "\n",
       "        month  day  \n",
       "0          12    1  \n",
       "1          12    2  \n",
       "2          12    3  \n",
       "3          12    4  \n",
       "4          12    5  \n",
       "...       ...  ...  \n",
       "142188      6   20  \n",
       "142189      6   21  \n",
       "142190      6   22  \n",
       "142191      6   23  \n",
       "142192      6   24  \n",
       "\n",
       "[142193 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142193 entries, 0 to 142192\n",
      "Data columns (total 24 columns):\n",
      "Location         142193 non-null object\n",
      "MinTemp          141556 non-null float64\n",
      "MaxTemp          141871 non-null float64\n",
      "Rainfall         140787 non-null float64\n",
      "Evaporation      81350 non-null float64\n",
      "Sunshine         74377 non-null float64\n",
      "WindGustDir      132863 non-null object\n",
      "WindGustSpeed    132923 non-null float64\n",
      "WindDir9am       132180 non-null object\n",
      "WindDir3pm       138415 non-null object\n",
      "WindSpeed9am     140845 non-null float64\n",
      "WindSpeed3pm     139563 non-null float64\n",
      "Humidity9am      140419 non-null float64\n",
      "Humidity3pm      138583 non-null float64\n",
      "Pressure9am      128179 non-null float64\n",
      "Pressure3pm      128212 non-null float64\n",
      "Cloud9am         88536 non-null float64\n",
      "Cloud3pm         85099 non-null float64\n",
      "Temp9am          141289 non-null float64\n",
      "Temp3pm          139467 non-null float64\n",
      "RainToday        140787 non-null object\n",
      "year             142193 non-null int64\n",
      "month            142193 non-null int64\n",
      "day              142193 non-null int64\n",
      "dtypes: float64(16), int64(3), object(5)\n",
      "memory usage: 26.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeral_features = data.select_dtypes('float64').columns\n",
    "categorial_features = data.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним вещественные признаки средним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numeral_features:\n",
    "    data[column].fillna(data[column].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Категориальные модой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorial_features:\n",
    "    data[column].fillna(data[column].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142193 entries, 0 to 142192\n",
      "Data columns (total 24 columns):\n",
      "Location         142193 non-null object\n",
      "MinTemp          142193 non-null float64\n",
      "MaxTemp          142193 non-null float64\n",
      "Rainfall         142193 non-null float64\n",
      "Evaporation      142193 non-null float64\n",
      "Sunshine         142193 non-null float64\n",
      "WindGustDir      142193 non-null object\n",
      "WindGustSpeed    142193 non-null float64\n",
      "WindDir9am       142193 non-null object\n",
      "WindDir3pm       142193 non-null object\n",
      "WindSpeed9am     142193 non-null float64\n",
      "WindSpeed3pm     142193 non-null float64\n",
      "Humidity9am      142193 non-null float64\n",
      "Humidity3pm      142193 non-null float64\n",
      "Pressure9am      142193 non-null float64\n",
      "Pressure3pm      142193 non-null float64\n",
      "Cloud9am         142193 non-null float64\n",
      "Cloud3pm         142193 non-null float64\n",
      "Temp9am          142193 non-null float64\n",
      "Temp3pm          142193 non-null float64\n",
      "RainToday        142193 non-null object\n",
      "year             142193 non-null int64\n",
      "month            142193 non-null int64\n",
      "day              142193 non-null int64\n",
      "dtypes: float64(16), int64(3), object(5)\n",
      "memory usage: 26.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorial_features:\n",
    "    categories = pd.get_dummies(data[column])\n",
    "    categories.columns = [column + c for c in categories.columns]\n",
    "    data = pd.concat([data, categories], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorial_features:\n",
    "    data.drop([column], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[indexes['train']]\n",
    "X_val = X[indexes['val']]\n",
    "X_test = X[indexes['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(target_data == 'Yes', dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[indexes['train']]\n",
    "y_val = y[indexes['val']]\n",
    "y_test = y[indexes['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    assert y_true.shape[0] == y_pred.shape[0]\n",
    "    return (y_true == y_pred).sum() / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8564796007196332"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(rf.predict(X_val), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8443058443058443"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(rf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline пройден с первой попытки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8405199930358076"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(gb.predict(X_val), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8295603295603295"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(gb.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбираем n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = {}\n",
    "answer['n'] = []\n",
    "answer['acc'] = []\n",
    "n_esters = range(50, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in n_esters:\n",
    "    gb = GradientBoostingClassifier(n_estimators=k, warm_start=True)\n",
    "    gb.fit(X_train, y_train)\n",
    "    answer['n'].append(k)\n",
    "    answer['acc'].append(accuracy(gb.predict(X_val), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 675x450 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,6), dpi=75)\n",
    "plt.plot(answer['n'], answer['acc'])\n",
    "plt.xlabel('N_estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8574661946491788"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(gb.predict(X_val), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8479248479248479"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(gb.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
