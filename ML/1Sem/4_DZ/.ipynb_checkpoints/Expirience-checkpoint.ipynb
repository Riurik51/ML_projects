{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_finite_diff(function, X, y, w, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Возвращает численное значение градиента, подсчитанное по следующией формуле:\n",
    "        result_i := (f(w + eps * e_i) - f(w)) / eps,\n",
    "        где e_i - следующий вектор:\n",
    "        e_i = (0, 0, ..., 0, 1, 0, ..., 0)\n",
    "                          >> i <<\n",
    "    \"\"\"\n",
    "    f_w = function(X, y, w) * (-1)\n",
    "    result = np.ones(w.shape[0]) * f_w\n",
    "    for i in range(w.shape[0]):\n",
    "        w_e = np.copy(w)\n",
    "        w_e[i] += eps\n",
    "        result[i] += function(X, y, w_e)\n",
    "    return result / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class BaseSmoothOracle:\n",
    "    \"\"\"\n",
    "    Базовый класс для реализации оракулов.\n",
    "    \"\"\"\n",
    "    def func(self, w):\n",
    "        \"\"\"\n",
    "        Вычислить значение функции в точке w.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Func oracle is not implemented.')\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\"\n",
    "        Вычислить значение градиента функции в точке w.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Grad oracle is not implemented.')\n",
    "\n",
    "\n",
    "class BinaryLogistic(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Оракул для задачи двухклассовой логистической регрессии.\n",
    "    Оракул должен поддерживать l2 регуляризацию.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, l2_coef):\n",
    "        \"\"\"\n",
    "        Задание параметров оракула.\n",
    "        l2_coef - коэффициент l2 регуляризации\n",
    "        \"\"\"\n",
    "        self.l2_coef = l2_coef\n",
    "\n",
    "    def func(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Вычислить значение функционала в точке w на выборке X с ответами y.\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        w - одномерный numpy array\n",
    "        \"\"\"\n",
    "        return np.logaddexp(0, -X.dot(w) * y).mean() + \\\n",
    "            (self.l2_coef / 2) * np.sum(w ** 2)\n",
    "\n",
    "    def grad(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Вычислить градиент функционала в точке w на выборке X с ответами y.\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        w - одномерный numpy array\n",
    "        \"\"\"\n",
    "        return -(X.transpose().dot(y * expit(-X.dot(w) * y))) / y.shape[0] + \\\n",
    "                self.l2_coef * w\n",
    "\n",
    "\n",
    "class GDClassifier:\n",
    "    \"\"\"\n",
    "    Реализация метода градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_function, step_alpha=1, step_beta=0,\n",
    "                 tolerance=1e-5, max_iter=1000, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора.\n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "        tolerance - точность, по достижении которой,\n",
    "        необходимо прекратить оптимизацию.\n",
    "        Необходимо использовать критерий выхода\n",
    "        по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход\n",
    "        max_iter - максимальное число итераций\n",
    "        **kwargs - аргументы, необходимые для инициализации\n",
    "        \"\"\"\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        if loss_function == 'binary_logistic':\n",
    "            self.Loss = BinaryLogistic(**kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError('Loss function is not implemented')\n",
    "\n",
    "    def fit(self, X, y, w_0=None, trace=False, X_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        w_0 - начальное приближение в методе\n",
    "        trace - переменная типа bool\n",
    "        Если trace = True, то метод должен вернуть словарь history,\n",
    "        содержащий информацию о поведении метода.\n",
    "        Длина словаря history = количество итераций + 1 (начальное приближение)\n",
    "        history['time']: list of floats, содержит интервалы времени\n",
    "        между двумя итерациями метода history['func']: list of floats,\n",
    "        содержит значения функции на каждой итерации\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        if w_0 is None:\n",
    "            w_0 = np.zeros(X.shape[1])\n",
    "        w = np.copy(w_0)\n",
    "        self.w = w\n",
    "        pred_loss = self.Loss.func(X, y, w)\n",
    "        if trace is True:\n",
    "            history = {}\n",
    "            history['time'] = []\n",
    "            history['func'] = []\n",
    "            time_pred = time.time()\n",
    "            history['time'].append(0)\n",
    "            history['func'].append(pred_loss)\n",
    "            if X_test is not None and y_test is not None:\n",
    "                history['test_acc'] = []\n",
    "                history['train_acc'] = []\n",
    "                history['test_acc'].append(float((self.predict(X_test) == y_test).sum()) / y_test.shape[0])\n",
    "                history['train_acc'].append(float((self.predict(X) == y).sum()) / y.shape[0])\n",
    "        for i in range(1, self.max_iter + 1):\n",
    "            w_next = w - self.Loss.grad(X, y, w) * ((self.step_alpha) /\n",
    "                                                    (i ** self.step_beta))\n",
    "            new_loss = self.Loss.func(X, y, w_next)\n",
    "            if np.abs(new_loss - pred_loss) < self.tolerance:\n",
    "                self.w = w_next\n",
    "                if trace is True:\n",
    "                    time_next = time.time()\n",
    "                    history['time'].append(time_next - time_pred)\n",
    "                    history['func'].append(new_loss)\n",
    "                    if X_test is not None and y_test is not None:\n",
    "                        history['test_acc'].append(float((self.predict(X_test) == y_test).sum()) / y_test.shape[0])\n",
    "                        history['train_acc'].append(float((self.predict(X) == y).sum()) / y.shape[0])\n",
    "                    return history\n",
    "                break\n",
    "            w = w_next\n",
    "            self.w = w\n",
    "            if trace is True:\n",
    "                time_next = time.time()\n",
    "                history['time'].append(time_next - time_pred)\n",
    "                time_pred = time_next\n",
    "                history['func'].append(new_loss)\n",
    "                if X_test is not None and y_test is not None:\n",
    "                    history['test_acc'].append(float((self.predict(X_test) == y_test).sum()) / y_test.shape[0])\n",
    "                    history['train_acc'].append(float((self.predict(X) == y).sum()) / y.shape[0])\n",
    "            pred_loss = new_loss\n",
    "        if trace is True:\n",
    "            return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Получение меток ответов на выборке X\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        return: одномерный numpy array с предсказаниями\n",
    "        \"\"\"\n",
    "        return np.sign(X.dot(self.w))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Получение вероятностей принадлежности X к классу k\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        return: двумерной numpy array,\n",
    "        [i, k] значение соответветствует вероятности\n",
    "        принадлежности i-го объекта к классу k\n",
    "        \"\"\"\n",
    "        answer = np.zeros((X.shape[0], 2), float)\n",
    "        answer[:, 1] = expit(X.dot(self.w))\n",
    "        answer[:, 0] = -answer[:, 1] + 1\n",
    "        return answer\n",
    "\n",
    "    def get_objective(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения целевой функции на выборке X с ответами y\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        return: float\n",
    "        \"\"\"\n",
    "        return self.Loss.func(X, y, self.w)\n",
    "\n",
    "    def get_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения градиента функции на выборке X с ответами y\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        return: numpy array, размерность зависит от задачи\n",
    "        \"\"\"\n",
    "        return self.Loss.grad(X, y, self.w)\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Получение значения весов функционала\n",
    "        \"\"\"\n",
    "        return self.w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((300, 2))\n",
    "X[:150] = np.random.rand(150, 2)\n",
    "X[150:] = np.random.rand(150, 2) - 0.5\n",
    "y = np.zeros(300, int)\n",
    "y[:150] = 1\n",
    "y[150:] = -1\n",
    "X_test = np.zeros((100,2))\n",
    "X_test[:50] = np.random.rand(50, 2) \n",
    "X_test[50:] = np.random.rand(50, 2)  - 0.5\n",
    "y_test = np.zeros(100, int)\n",
    "y_test[:50] = 1\n",
    "y_test[50:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdc = GDClassifier('binary_logistic', l2_coef=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = gdc.fit(X, y, trace=True,  X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDClassifier(GDClassifier):\n",
    "    \"\"\"\n",
    "    Реализация метода стохастического градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_function, batch_size, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, random_seed=153, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "        \n",
    "        batch_size - размер подвыборки, по которой считается градиент\n",
    "        \n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "        \n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "        \n",
    "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию\n",
    "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход \n",
    "        \n",
    "        \n",
    "        max_iter - максимальное число итераций (эпох)\n",
    "        \n",
    "        random_seed - в начале метода fit необходимо вызвать np.random.seed(random_seed).\n",
    "        Этот параметр нужен для воспроизводимости результатов на разных машинах.\n",
    "        \n",
    "        **kwargs - аргументы, необходимые для инициализации\n",
    "        \"\"\"\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.random_seed = random_seed\n",
    "        self.batch_size = batch_size\n",
    "        if loss_function == 'binary_logistic':\n",
    "            self.Loss = BinaryLogistic(**kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError('Loss function is not implemented')\n",
    "        \n",
    "    def fit(self, X, y, w_0=None, trace=False, log_freq=1):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "                \n",
    "        w_0 - начальное приближение в методе\n",
    "        \n",
    "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
    "        о поведении метода. Если обновлять history после каждой итерации, метод перестанет \n",
    "        превосходить в скорости метод GD. Поэтому, необходимо обновлять историю метода лишь\n",
    "        после некоторого числа обработанных объектов в зависимости от приближённого номера эпохи.\n",
    "        Приближённый номер эпохи:\n",
    "            {количество объектов, обработанных методом SGD} / {количество объектов в выборке}\n",
    "        \n",
    "        log_freq - float от 0 до 1, параметр, отвечающий за частоту обновления. \n",
    "        Обновление должно проиходить каждый раз, когда разница между двумя значениями приближённого номера эпохи\n",
    "        будет превосходить log_freq.\n",
    "        \n",
    "        history['epoch_num']: list of floats, в каждом элементе списка будет записан приближённый номер эпохи:\n",
    "        history['time']: list of floats, содержит интервалы времени между двумя соседними замерами\n",
    "        history['func']: list of floats, содержит значения функции после текущего приближённого номера эпохи\n",
    "        history['weights_diff']: list of floats, содержит квадрат нормы разности векторов весов с соседних замеров\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_seed)\n",
    "        if w_0 is None:\n",
    "            w_0 = np.zeros(X.shape[1])\n",
    "        w = np.copy(w_0)\n",
    "        w_next = w\n",
    "        pred_loss = self.Loss.func(X, y, w)\n",
    "        epoch_num_pred = 0.0\n",
    "        epoch_num = epoch_num_pred\n",
    "        if trace is True:\n",
    "            history = {}\n",
    "            history['epoch_num'] = []\n",
    "            history['time'] = []\n",
    "            history['func'] = []\n",
    "            history['weights_diff'] = []\n",
    "            time_pred = time.time()\n",
    "            history['time'].append(0)\n",
    "            history['func'].append(pred_loss)\n",
    "            history['epoch_num'].append(epoch_num_pred)\n",
    "            history['weights_diff'].append(0.0)\n",
    "        for i in range(self.max_iter):\n",
    "            diff = float(self.batch_size) / X.shape[0]\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            w_pred = w\n",
    "            for j in range(self.batch_size, X.shape[0], self.batch_size):\n",
    "                w_next = w - self.Loss.grad(X[j - self.batch_size:j], y[j - self.batch_size:j], w) * \\\n",
    "                        ((self.step_alpha) / (i ** self.step_beta))\n",
    "                epoch_num += diff\n",
    "                w = w_next\n",
    "                if epoch_num - epoch_num_pred > log_freq:\n",
    "                    break\n",
    "            epoch_num_pred = epoch_num\n",
    "            new_loss = self.Loss.func(X, y, w_next)\n",
    "            if abs(new_loss - pred_loss) < self.tolerance:\n",
    "                if trace is True:\n",
    "                    time_new = time.time()\n",
    "                    history['time'].append(time_new - time_pred)\n",
    "                    history['func'].append(new_loss)\n",
    "                    history['epoch_num'].append(epoch_num)\n",
    "                    history['weights_diff'].append(np.linalg.norm(w - w_pred))\n",
    "                    self.w = w\n",
    "                    return history\n",
    "                break\n",
    "            pred_loss = new_loss\n",
    "            if trace is True:\n",
    "                    time_new = time.time()\n",
    "                    history['time'].append(time_new - time_pred)\n",
    "                    time_pred = time_new\n",
    "                    history['func'].append(new_loss)\n",
    "                    history['epoch_num'].append(epoch_num)\n",
    "                    history['weights_diff'].append(np.linalg.norm(w - w_pred))\n",
    "        self.w = w\n",
    "        if trace is True:\n",
    "            return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier('binary_logistic', batch_size=1, l2_coef=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = sgd.fit(X, y, w_0=np.random.rand(5), trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5330269124737894,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " 46.75933332444282,\n",
       " 0.6931471805599453,\n",
       " ...]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi['func']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, w_0=None, trace=False, X_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        w_0 - начальное приближение в методе\n",
    "        trace - переменная типа bool\n",
    "        Если trace = True, то метод должен вернуть словарь history,\n",
    "        содержащий информацию о поведении метода.\n",
    "        Длина словаря history = количество итераций + 1 (начальное приближение)\n",
    "        history['time']: list of floats, содержит интервалы времени\n",
    "        между двумя итерациями метода history['func']: list of floats,\n",
    "        содержит значения функции на каждой итерации\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        if w_0 is None:\n",
    "            w_0 = np.zeros(X.shape[1])\n",
    "        w = np.copy(w_0)\n",
    "        self.w = w\n",
    "        pred_loss = self.Loss.func(X, y, w)\n",
    "        if trace is True:\n",
    "            history = {}\n",
    "            history['time'] = []\n",
    "            history['func'] = []\n",
    "            time_pred = time.time()\n",
    "            history['time'].append(0)\n",
    "            history['func'].append(pred_loss)\n",
    "            if X_test is not None and y_test is not None:\n",
    "                history['test_acc'] = []\n",
    "                history['train_acc'] = []\n",
    "                history['test_acc'].append(float((self.predict(X_test) == y_test).sum()) / y_test.shape[0])\n",
    "                history['train_acc'].append(float((self.predict(X) == y).sum()) / y.shape[0])\n",
    "        for i in range(1, self.max_iter + 1):\n",
    "            w_next = w - self.Loss.grad(X, y, w) * ((self.step_alpha) /\n",
    "                                                    (i ** self.step_beta))\n",
    "            new_loss = self.Loss.func(X, y, w_next)\n",
    "            if np.abs(new_loss - pred_loss) < self.tolerance:\n",
    "                self.w = w_next\n",
    "                if trace is True:\n",
    "                    time_next = time.time()\n",
    "                    history['time'].append(time_next - time_pred)\n",
    "                    history['func'].append(new_loss)\n",
    "                    if X_test is not None and y_test is not None:\n",
    "                        history['test_acc'].append(float((self.predict(X_test) == y_test).sum()) / y_test.shape[0])\n",
    "                        history['train_acc'].append(float((self.predict(X) == y).sum()) / y.shape[0])\n",
    "                    return history\n",
    "                break\n",
    "            w = w_next\n",
    "            self.w = w\n",
    "            if trace is True:\n",
    "                time_next = time.time()\n",
    "                history['time'].append(time_next - time_pred)\n",
    "                time_pred = time_next\n",
    "                history['func'].append(new_loss)\n",
    "                if X_test is not None and y_test is not None:\n",
    "                    history['test_acc'].append(float((self.predict(X_test) == y_test).sum()) / y_test.shape[0])\n",
    "                    history['train_acc'].append(float((self.predict(X) == y).sum()) / y.shape[0])\n",
    "            pred_loss = new_loss\n",
    "        if trace is True:\n",
    "            return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
