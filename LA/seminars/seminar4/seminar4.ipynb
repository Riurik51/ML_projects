{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Семинар 4.\n",
    "## Линейная алгебра и нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Нейросеть как суперпозиция функций\n",
    "\n",
    "- Линейных и нелинейных\n",
    "- Можно представить в виде графа вычислений\n",
    "- С помощью этого графа можно [автоматически вычислять градиенты](https://en.wikipedia.org/wiki/Automatic_differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Что такое градиент и как его вычислить?\n",
    "\n",
    "- Если функция $f$ зависит от вектора или матрицы и вычисляет скаляр, то её градиент по аргументу - это вектор или матрица, элементы которых – производные выхода по соответствующему элементу аргумента\n",
    "- Пример: $f(x) = x^{\\top}Ax + b^{\\top}x$. Чему равен градиент?\n",
    "- Поэлементный пример: $f(x) = x^2$ поэлементно. Чему равен градиент (матрица Якоби)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1100, grad_fn=<SubBackward0>)\n",
      "-0.10995006561279297\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n = 5\n",
    "A = torch.randn((n, n), requires_grad=True)\n",
    "b = torch.randn((n,))\n",
    "x = torch.randn((n,), requires_grad=True)\n",
    "\n",
    "f = 0.5 * x @ A @ x - b @ x\n",
    "f.backward()\n",
    "print(f)\n",
    "print(f.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0192, -0.6071, -0.0283, -0.8360, -2.2871])\n",
      "tensor([-1.0192, -0.6071, -0.0283, -0.8360, -2.2871])\n"
     ]
    }
   ],
   "source": [
    "manual_grad_x = 0.5 * (A + A.t()) @ x - b\n",
    "\n",
    "print(manual_grad_x.data)\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0132,  0.9357,  0.1772, -0.1832, -0.2767],\n",
      "        [ 0.9357,  0.4349,  0.0824, -0.0852, -0.1286],\n",
      "        [ 0.1772,  0.0824,  0.0156, -0.0161, -0.0244],\n",
      "        [-0.1832, -0.0852, -0.0161,  0.0167,  0.0252],\n",
      "        [-0.2767, -0.1286, -0.0244,  0.0252,  0.0380]])\n",
      "tensor([[ 2.0132,  0.9357,  0.1772, -0.1832, -0.2767],\n",
      "        [ 0.9357,  0.4349,  0.0824, -0.0852, -0.1286],\n",
      "        [ 0.1772,  0.0824,  0.0156, -0.0161, -0.0244],\n",
      "        [-0.1832, -0.0852, -0.0161,  0.0167,  0.0252],\n",
      "        [-0.2767, -0.1286, -0.0244,  0.0252,  0.0380]])\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "manual_grad_A = 0.5 * torch.ger(x, x)\n",
    "\n",
    "print(manual_grad_A.data)\n",
    "print(A.grad.data)\n",
    "print(torch.norm(manual_grad_A.data - A.grad.data).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Причём тут нейросети?\n",
    "\n",
    "- Обучение с учителем (но не только!)\n",
    "- Есть данные, есть ответы\n",
    "- Нейросеть $\\approx$ сложная композиция простых функций, которая по данным восстанавливает ответы\n",
    "- Для обучения необходима целевая функция a.k.a. функция потерь или loss, который минимизируется каким-нибудь стохастическим методом первого порядка\n",
    "- Поэтому нужны градиенты!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./pytorch_logo.png\">\n",
    "\n",
    "- Удобный фреймворк для построения и обучения нейросетей\n",
    "- Реализует динамический граф вычислений\n",
    "- Вы просто пишете функцию, которая вычисляет нужное вам выражение, вычисляете (вызываете) её в некоторой точке (с некоторым аргументом) и можете получить градиенты в этой точке бесплатно!\n",
    "- Специальные инструменты для построения нейросетей: модуль с большинством популярных слоёв, для которых градиенты уже реализованы\n",
    "- Пока плохо поддерживает операции с разреженными матрицами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU vs. CPU\n",
    "\n",
    "- CPU – мощный вычислитель, но мало памяти доступно здесь и сейчас (см. лекцию про иерархию памяти)\n",
    "- GPU – ОЧЕНЬ много ядер для выполнения простых операций\n",
    "- CPU распределяет задачи по ядрам, GPU - по кластерам ядер\n",
    "\n",
    "Только использование GPU (или [TPU](https://habr.com/ru/post/422317/)) позволит за разумное время обучить сложные модели для задач компьютерного зрения, NLP, RL и других областей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Какие операции нужны для работы нейросетей?\n",
    "\n",
    "- Умножение матрицы на вектор и векторное сложение - полносвязный слой\n",
    "- Поэлементые операции - нелинейности\n",
    "- Локальные операции типа [MaxPooling](https://deepai.org/machine-learning-glossary-and-terms/max-pooling)\n",
    "- Получение из вектора или матрицы скаляра"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пример реализации операции матричного умножения\n",
    "\n",
    "- Пример исходного кода, реализующего матричное умножение на CUDA, доступен [тут](https://www.quantstart.com/articles/Matrix-Matrix-Multiplication-on-the-GPU-with-Nvidia-CUDA)\n",
    "- Сравнение с помощью PyTorch доступно по ссылке ниже [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1aLpXukb3p-hzENDVaqBHltjRFfwpJ6f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Резюме\n",
    "\n",
    "- В основе нейросетей как и других выислительных технологий лежит линейная алгебра\n",
    "- Современные фреймворки (PyTorch, etc) позволяют вычислять градиенты автоматически\n",
    "- GPU быстрее CPU для выполнения большого числа простых операций\n",
    "- TPU ещё быстрее, но менее доступны\n",
    "- [Colab](https://colab.research.google.com/) позволяет экспериментировать с этими технологиями удалённо"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
